{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\ntorch.manual_seed(0)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.004184Z","iopub.execute_input":"2022-07-01T06:33:25.005092Z","iopub.status.idle":"2022-07-01T06:33:25.032551Z","shell.execute_reply.started":"2022-07-01T06:33:25.004940Z","shell.execute_reply":"2022-07-01T06:33:25.031010Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/landmark1/keypoint.csv', header = None)\nprint(data.shape)\ndata[0].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.035291Z","iopub.execute_input":"2022-07-01T06:33:25.035809Z","iopub.status.idle":"2022-07-01T06:33:25.165012Z","shell.execute_reply.started":"2022-07-01T06:33:25.035759Z","shell.execute_reply":"2022-07-01T06:33:25.164051Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\ntrain_data, test_data = train_test_split(data, test_size = 0.2, random_state = 0, stratify = data[0])\ntrain_data.reset_index(drop=True, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)\ntrain_data.shape, test_data.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.167114Z","iopub.execute_input":"2022-07-01T06:33:25.167673Z","iopub.status.idle":"2022-07-01T06:33:25.185780Z","shell.execute_reply.started":"2022-07-01T06:33:25.167641Z","shell.execute_reply":"2022-07-01T06:33:25.184624Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"torch.tensor(train_data.values[:,0], dtype = torch.float)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.187379Z","iopub.execute_input":"2022-07-01T06:33:25.187734Z","iopub.status.idle":"2022-07-01T06:33:25.199714Z","shell.execute_reply.started":"2022-07-01T06:33:25.187703Z","shell.execute_reply":"2022-07-01T06:33:25.198863Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class LandmarkDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, index):\n        target = torch.tensor(self.df.values[:, 0]).type(torch.LongTensor)\n        features = torch.tensor(self.df.values[:, 1:], dtype = torch.float)\n        return features[index], target[index]\n    \n    \ntrain_set = LandmarkDataset(train_data)\ntest_set = LandmarkDataset(test_data)\nlen(train_set), len(test_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.202259Z","iopub.execute_input":"2022-07-01T06:33:25.202603Z","iopub.status.idle":"2022-07-01T06:33:25.214606Z","shell.execute_reply.started":"2022-07-01T06:33:25.202572Z","shell.execute_reply":"2022-07-01T06:33:25.213763Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainloader = DataLoader(train_set, batch_size = 32, shuffle = True)\ntestloader = DataLoader(test_set, batch_size = 32, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.216187Z","iopub.execute_input":"2022-07-01T06:33:25.216745Z","iopub.status.idle":"2022-07-01T06:33:25.228440Z","shell.execute_reply.started":"2022-07-01T06:33:25.216712Z","shell.execute_reply":"2022-07-01T06:33:25.226957Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x, y = iter(trainloader).next()\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.230076Z","iopub.execute_input":"2022-07-01T06:33:25.230809Z","iopub.status.idle":"2022-07-01T06:33:25.283810Z","shell.execute_reply.started":"2022-07-01T06:33:25.230764Z","shell.execute_reply":"2022-07-01T06:33:25.282611Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"n_features = 42\nhidden_size = [32, 16]\nn_classes = len(data[0].unique())\nclass MLP(nn.Module):\n    def __init__(self, n_features, n_classes, hidden_size):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(n_features, hidden_size[0])\n        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n        self.output = nn.Linear(hidden_size[1], n_classes)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return F.log_softmax(self.output(x), dim = 1)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.286183Z","iopub.execute_input":"2022-07-01T06:33:25.287165Z","iopub.status.idle":"2022-07-01T06:33:25.300694Z","shell.execute_reply.started":"2022-07-01T06:33:25.287109Z","shell.execute_reply":"2022-07-01T06:33:25.299268Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = MLP(n_features, n_classes, hidden_size)\nmodel.to(device)\nlearning_rate = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr= learning_rate)\ncriterion = nn.NLLLoss()\n\ntrain_losses = []\ntest_losses = []\ntrain_accuracies = []\ntest_accuracies = []\nbenchmark_accuracy = 0.95\nfor epoch in range(epochs):\n    print(f'Epoch {epoch + 1}/{epochs}')\n    running_accuracy = 0\n    running_loss = 0\n    # training\n    for x_train_batch, y_train_batch in trainloader:\n        x_train_batch = x_train_batch.to(device)\n        y_train_batch = y_train_batch.to(device)\n\n        optimizer.zero_grad()\n\n        # forward pass\n        scores = model(x_train_batch)\n        train_preds = torch.argmax(scores.detach(), dim=1)\n\n        # loss\n        train_loss = criterion(scores, y_train_batch)\n        running_loss += train_loss.item()\n\n        # train accuracy\n        train_acc = (y_train_batch == train_preds).sum() / len(y_train_batch)\n        running_accuracy += train_acc.item()\n\n        # backward pass\n        \n        train_loss.backward()\n        \n        # update paramaters\n        \n        optimizer.step()\n\n    # mean loss (all batches losses divided by the total number of batches)\n    train_losses.append(running_loss / len(trainloader))\n    \n    # mean accuracies\n    train_accuracies.append(running_accuracy / len(trainloader))\n    \n    # print\n    print(f'Train loss: {train_losses[-1] :.4f}')\n\n    # validation\n    model.eval()\n    with torch.no_grad():\n        running_accuracy = 0\n        running_loss = 0\n\n        for x_test_batch, y_test_batch in testloader:\n            x_test_batch = x_test_batch.to(device)\n            y_test_batch = y_test_batch.to(device)\n            # logits\n            test_scores = model(\n                x_test_batch)\n\n            # predictions\n            test_preds = torch.argmax(test_scores, dim=1)\n            \n            # accuracy\n            test_acc = (y_test_batch == test_preds).sum() / len(y_test_batch)\n            running_accuracy += test_acc.item()\n\n            # loss\n            test_loss = criterion(test_scores, y_test_batch)\n            running_loss += test_loss.item()\n\n        # mean accuracy for each epoch\n        test_accuracies.append(running_accuracy / len(testloader))\n\n        # mean loss for each epoch\n        test_losses.append(running_accuracy / len(testloader))\n        # print\n        print(f'Test accuracy: {test_accuracies[-1]*100 :.2f}%')\n        print('='*100)\n        # saving best model\n        # is current mean score (mean per epoch) greater than or equal to the benchmark?\n        if test_accuracies[-1] > benchmark_accuracy:\n            # save model to cpu\n            torch.save(model.to('cpu').state_dict(), './model.pth')\n            model.to(device) # bring back to gpu\n\n            # update benckmark\n            benchmark_accuracy = test_accuracies[-1]\n\n    model.train()\n\n\n# Plots\nx_epochs = list(range(epochs))\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.plot(x_epochs, train_losses, marker='o', label='train')\nplt.plot(x_epochs, test_losses, marker='o', label='test')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x_epochs, train_accuracies, marker='o', label='train')\nplt.plot(x_epochs, test_accuracies, marker='o', label='test')\nplt.axhline(benchmark_accuracy, c='grey', ls='--',\n            label=f'Best_accuracy({benchmark_accuracy*100 :.2f}%)')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.savefig('./learning_curve.png', dpi = 200)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:33:25.302139Z","iopub.execute_input":"2022-07-01T06:33:25.303050Z","iopub.status.idle":"2022-07-01T06:35:28.556846Z","shell.execute_reply.started":"2022-07-01T06:33:25.303004Z","shell.execute_reply":"2022-07-01T06:35:28.555027Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}